{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This challenge focuses on extracting meaning from text. Use cases in a finance company could include:\n",
    "\n",
    "Automatic chatbots\n",
    "Classifying customer complaints and communications\n",
    "Automated underwriting from medical records\n",
    "Automated claims handling from accident reports and call transcripts\n",
    "Here we will use data from government e-petitions to explore two common uses of text in Data Science:\n",
    "\n",
    "Predictive Modelling\n",
    "Topic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Importing packages we might need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import resample\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "import pyLDAvis\n",
    "import plotly.express as px\n",
    "from sklearn.utils import resample\n",
    "from operator import itemgetter\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_data.json', 'rb') as f:\n",
    "    training_data = json.load(f)\n",
    "\n",
    "with open('holdout_data.json', 'rb') as f:\n",
    "    holdout_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first petition in our training set, and have a look at how many petitions and signatures we're dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(training_data[0])\n",
    "\n",
    "print(\"\\nNumber of petitions in training data: {}\".format(len(training_data)))\n",
    "print(\"\\nMean number of signatures: {}\".format(int(np.mean([p['numberOfSignatures'] for p in training_data]))))\n",
    "print(\"\\nMedian number of signatures: {}\".format(int(np.median([p['numberOfSignatures'] for p in training_data]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'abstract': {'_value': 'MPs should attend all debates, not merely turn up and vote or strike pairing deals. With other commitments, a five day Commons is not workable for MPs: I suggest three full days (9am to 6pm minimum), with one or two days for Committees, leaving at least one day for constituency work.'}, 'created': {'_value': '2016-03-15T15:56:53.752Z', '_datatype': 'dateTime'}, 'label': {'_value': 'Reform the Commons: Three days full time with compulsory attendance for all MPs.'}, 'numberOfSignatures': 27, 'status': 'closed'}\n",
    "\n",
    "Number of petitions in training data: 12387\n",
    "\n",
    "Mean number of signatures: 3777\n",
    "\n",
    "Median number of signatures: 58\n",
    "We can see that the petition text is stored under two keys - the value of label gives the petition title, and the value of abstract provides a longer description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Cleaning the Text\n",
    "Usually in a Data Science problem we would start with Exploratory Data Analysis (EDA). But this is unstructured text - we can't calculate simple statistics or plot interesting histograms until we have turned this text into numbers. Before thinking about making a model, structure and clean the text.\n",
    "\n",
    "\n",
    "You could consider:\n",
    "\n",
    "Tokenizing\n",
    "Lemmatizing or Stemming\n",
    "Filtering\n",
    "Calculating TF-IDF\n",
    "Packages of use may include:\n",
    "\n",
    "NLTK\n",
    "Gensim\n",
    "Sklearn\n",
    "Any blog post about any form of text modelling will begin with a section on text preprocessing. Some examples are here:\n",
    "\n",
    "https://medium.com/@annabiancajones/sentiment-analysis-of-reviews-text-pre-processing-6359343784fb\n",
    "https://www.machinelearningplus.com/nlp/gensim-tutorial/#8howtocreatethetfidfmatrixcorpusingensim\n",
    "https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\n",
    "https://towardsdatascience.com/nlp-extracting-the-main-topics-from-your-dataset-using-lda-in-minutes-21486f5aa925\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petition_list = []\n",
    "for petition in training_data:\n",
    "    title = petition['label']['_value']\n",
    "    abstract = petition['abstract']['_value']\n",
    "    sign = petition['numberOfSignatures']\n",
    "    petition_list.append([title,abstract,sign])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petition_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petition_texts = []\n",
    "for petition in training_data:\n",
    "    title = petition['label']['_value']\n",
    "    abstract = petition['abstract']['_value']\n",
    "    full_text = title + ' ' + abstract\n",
    "    petition_texts.append(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petition_texts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk, re\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "normalizer = WordNetLemmatizer()\n",
    "\n",
    "def get_part_of_speech(word):\n",
    "    probable_part_of_speech = wordnet.synsets(word)\n",
    "    pos_counts = Counter()\n",
    "    pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "    pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "    pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "    pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "    most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "    return most_likely_part_of_speech\n",
    "\n",
    "def preprocess_text(text):\n",
    "    cleaned = re.sub(r'\\W+', ' ', text).lower()\n",
    "    tokenized = word_tokenize(cleaned)\n",
    "    normalized = [normalizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((petition_list[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_petition_texts = [preprocess_text(text) for text in petition_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_petition_texts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting together the cleaned up words into one single string so that tf-idf can accept it\n",
    "\n",
    "cleaned_sentences = []\n",
    "for item in processed_petition_texts:\n",
    "    concat=''\n",
    "    for word in item:\n",
    "        concat+=word+' '\n",
    "    cleaned_sentences.append(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sentences[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(norm=None)\n",
    "tfidf_scores = vectorizer.fit_transform(cleaned_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(tfidf_scores))\n",
    "print(tfidf_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(learning_method='online', n_components=10)\n",
    "lda_tfidf = lda.fit_transform(tfidf_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n~~~ Topics found by tf-idf LDA ~~~\")\n",
    "for topic_id, topic in enumerate(lda.components_):\n",
    "    message = \"Topic #{}: \".format(topic_id + 1)\n",
    "    message += \" \".join([vectorizer.get_feature_names()[i] for i in topic.argsort()])\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Part I: Predict Whether a Petition Surpasses 50 Signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to create a model to predict whether a petition has surpassed 50 signatures, using only the petition text as input.\n",
    "\n",
    "\n",
    "Your first task will probably be feature generation. You may wish to consider:\n",
    "\n",
    "Word counts\n",
    "Word frequencies (and TF-IDF)\n",
    "Word embedding\n",
    "Custom rules\n",
    "This is a supervised learning task using text - multiple blog posts cover walkthroughs for various purposes (e.g. sentiment analysis). Some potential resources are here:\n",
    "\n",
    "https://medium.com/@annabiancajones/sentiment-analysis-on-reviews-feature-extraction-and-logistic-regression-43a29635cc81\n",
    "https://www.kaggle.com/arunava21/word2vec-and-random-forest-classification\n",
    "\n",
    "Once you have a model, predict whether each petition in the holdout set will surpass 50 signatures. Your predictions will be assessed against the truth using the F1 score. F1 accounts for both Precision (of all the petitions you predict to surpass 50 signatures, how often were you correct) and Recall (of all the petitions which surpass 50 signatures, how many do you correctly identify).\n",
    "\n",
    "\n",
    "Your submission for Part I should be a CSV list of 3,000 Booleans to represent your predictions for the holdout set - True if you think a petition will surpass 50 signatures, False otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Part II: Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many problems it is useful to cluster texts together which seem to talk about the same topic. You may wish to understand what customers tend to complain about, what news articles tend to be about or what reviews tend to talk about. For this task, you need to automatically group the petitions into topics..\n",
    "\n",
    "\n",
    "On the hack day you will show us your topic classification and explain how you have decided - quantiatively or qualitatively - on your end result.\n",
    "\n",
    "\n",
    "You may wish to consider:\n",
    "\n",
    "LDA\n",
    "Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
